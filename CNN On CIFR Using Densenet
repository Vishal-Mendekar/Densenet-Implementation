import tensorflow as tf
from tensorflow.keras import models, layers
from tensorflow.keras.models import Model
from tensorflow.keras.layers import BatchNormalization, Activation, Flatten, Conv2D
from tensorflow.keras.optimizers import Adam

# Hyperparameters
batch_size = 128
num_classes = 10
epochs = 175
l = 8
num_filter = 30
compression = 0.93

# Load CIFAR10 Data
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()
img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]

# convert to one hot encoing 
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes) 

from sklearn.model_selection import train_test_split
X_train, X_cv,y_train , y_cv = train_test_split(X_train, y_train, test_size = 0.25, stratify=y_train)
X_train.shape, y_train.shape
X_cv.shape, y_cv.shape
X_test.shape, y_test.shape

#Data Augmentation
# Source : https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator
from tensorflow.keras.preprocessing.image import ImageDataGenerator
generator = ImageDataGenerator(featurewise_center = True, featurewise_std_normalization =True, rotation_range=20, width_shift_range =0.2, height_shift_range = 0.2, horizontal_flip=True)

generator.fit(X_train)
train_data = generator.flow(X_train, y_train, batch_size=batch_size)
cv_data = generator.flow(X_cv, y_cv, batch_size=batch_size)

# Dense Block
# Removed Dropout Layer
# Replaced Dense Layer With Conv2D Layer with filters=10

def denseblock(input, num_filter = 30, dropout_rate = 0.0):
    global compression
    temp = input
    for _ in range(l): 
        BatchNorm = layers.BatchNormalization()(temp)
        relu = layers.Activation('relu')(BatchNorm)
        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)
        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])
        temp = concat
        
    return temp

## transition Blosck
def transition(input, num_filter = 30, dropout_rate = 0.0):
    global compression
    BatchNorm = layers.BatchNormalization()(input)
    relu = layers.Activation('relu')(BatchNorm)
    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)
    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)
    return avg

#output layer
def output_layer(input):
    global compression
    BatchNorm = layers.BatchNormalization()(input)
    relu = layers.Activation('relu')(BatchNorm)
    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)
    conv_layer = layers.Conv2D(filters=num_classes, kernel_size=(1,1),strides = (2,2),name='MyConv_Layer')(AvgPooling)
    per_final = layers.Activation('softmax')(conv_layer)
    output = layers.Flatten(name='Finally_Flatten')(per_final)
    return output
    
import numpy as np
num_filter = 30
dropout_rate = 0.00
l = 8
input = layers.Input(shape=(img_height, img_width, channel,))
First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)

First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)
First_Transition = transition(First_Block, num_filter, dropout_rate)

Second_Block = denseblock(First_Transition, num_filter, dropout_rate)
Second_Transition = transition(Second_Block, num_filter, dropout_rate)

Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)
Third_Transition = transition(Third_Block, num_filter, dropout_rate)

Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)
output = output_layer(Last_Block)

model = Model(inputs=[input], outputs=[output])
model.summary()


# determine Loss function and Optimizer
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import LearningRateScheduler
import datetime
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,write_graph=True, histogram_freq=1)
earlystop = EarlyStopping(monitor = 'val_accuracy', min_delta = 0.001, patience = 3, verbose = 1)
checkpoint_path = "/content/sample_data/Cnn weights"
checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True,verbose=1)
model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.01),metrics=['accuracy'])

def scheduler(epoch):
  if epoch < 20:
    return 0.0001
  else:
    return 0.001

lr_rate = LearningRateScheduler(scheduler)


model.fit(train_data,validation_data=cv_data, epochs = epochs, verbose=1, callbacks=[tensorboard_callback, checkpoint])

# Test the model
# Did this step as was not getting better accuracy
test_data = generator.flow(X_test, y_test, batch_size=batch_size)
score = model.evaluate(test_data, verbose=1)
print('Test loss:', score[0])
print('Test accuracy:', score[1]*100)

# Test the model
score = model.evaluate(X_test,y_test, verbose=1)
print('Test loss:', score[0])
print('Test accuracy:', score[1]*100)

%load_ext tensorboard
%tensorboard --logdir logs/fit

# Save the trained weights in to .h5 format
model.save_weights("DNST_model.h5")
print("Saved model to disk")

weights = model.load_weights(checkpoint_path)
____________________________________________________________________________________________________________________________________________________________
** Observation**

Epoch Accuracy was improving but there was continuos oscillation in accuracy.

The improvement rate was good at initial epochs as at 22nd epoch it gave 80+ validation accuracy.

The improvement rate degraded later because even after running for 175 epochs could not get 90+ validation accuracy.

Kernel Distribution remains sames through the 175 epochs.

Bias Distribution started widening with increase in epochs.

** Approach**

Augmented train, validation data initially.

Removed dropouts

Replaced Dense Layer with the equivalent Conv2D Layer.

As was not getting good test accuracy so along with evalaution on normal data have also evaluated on augmented test data

